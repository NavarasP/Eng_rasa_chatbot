{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar Errors: [Match({'ruleId': 'UPPERCASE_SENTENCE_START', 'message': 'This sentence does not start with an uppercase letter.', 'replacements': ['Or'], 'offsetInContext': 43, 'context': '...hrases for details on potential errors. or use this text too see an few of of the ...', 'offset': 168, 'errorLength': 2, 'category': 'CASING', 'ruleIssueType': 'typographical', 'sentence': 'or use this text too see an few of of the problems that LanguageTool can detecd.'}), Match({'ruleId': 'TOO_TO', 'message': 'Did you mean “to see”?', 'replacements': ['to see'], 'offsetInContext': 43, 'context': '...s on potential errors. or use this text too see an few of of the problems that Language...', 'offset': 185, 'errorLength': 7, 'category': 'CONFUSED_WORDS', 'ruleIssueType': 'misspelling', 'sentence': 'or use this text too see an few of of the problems that LanguageTool can detecd.'}), Match({'ruleId': 'EN_A_VS_AN', 'message': 'Use “a” instead of ‘an’ if the following word doesn’t start with a vowel sound, e.g. ‘a sentence’, ‘a university’.', 'replacements': ['a'], 'offsetInContext': 43, 'context': '...ential errors. or use this text too see an few of of the problems that LanguageToo...', 'offset': 193, 'errorLength': 2, 'category': 'MISC', 'ruleIssueType': 'misspelling', 'sentence': 'or use this text too see an few of of the problems that LanguageTool can detecd.'}), Match({'ruleId': 'ENGLISH_WORD_REPEAT_RULE', 'message': 'Possible typo: you repeated a word', 'replacements': ['of'], 'offsetInContext': 43, 'context': '...errors. or use this text too see an few of of the problems that LanguageTool can dete...', 'offset': 200, 'errorLength': 5, 'category': 'MISC', 'ruleIssueType': 'duplication', 'sentence': 'or use this text too see an few of of the problems that LanguageTool can detecd.'}), Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['detect'], 'offsetInContext': 43, 'context': '...f of the problems that LanguageTool can detecd. What do you thinks of grammar checkers...', 'offset': 241, 'errorLength': 6, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'or use this text too see an few of of the problems that LanguageTool can detecd.'}), Match({'ruleId': 'DO_VBZ', 'message': 'After the auxiliary verb ‘do’, use the base form of the main verb. Did you mean “think”?', 'replacements': ['think'], 'offsetInContext': 43, 'context': '...at LanguageTool can detecd. What do you thinks of grammar checkers? Please not that th...', 'offset': 261, 'errorLength': 6, 'category': 'GRAMMAR', 'ruleIssueType': 'grammar', 'sentence': 'What do you thinks of grammar checkers?'}), Match({'ruleId': 'PLEASE_NOT_THAT', 'message': 'Did you mean “note”?', 'replacements': ['note'], 'offsetInContext': 43, 'context': '... you thinks of grammar checkers? Please not that they are not perfect. Style issues...', 'offset': 296, 'errorLength': 3, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'Please not that they are not perfect.'}), Match({'ruleId': 'PM_IN_THE_EVENING', 'message': 'This is redundant. Consider using “P.M.”', 'replacements': ['P.M.'], 'offsetInContext': 43, 'context': '... Style issues get a blue marker: It’s 5 P.M. in the afternoon. The weather was nice on Thursday, 27 J...', 'offset': 366, 'errorLength': 21, 'category': 'REDUNDANCY', 'ruleIssueType': 'style', 'sentence': \"Style issues get a blue marker: It's 5 P.M. in the afternoon.\"}), Match({'ruleId': 'DATE_WEEKDAY', 'message': 'The date 27 June 2017 is not a Thursday, but a Tuesday.', 'replacements': [], 'offsetInContext': 43, 'context': '... the afternoon. The weather was nice on Thursday, 27 June 2017', 'offset': 413, 'errorLength': 22, 'category': 'SEMANTICS', 'ruleIssueType': 'inconsistency', 'sentence': 'The weather was nice on Thursday, 27 June 2017'})]\n",
      "Error: UPPERCASE_SENTENCE_START, Message: This sentence does not start with an uppercase letter., Suggestion: ['Or']\n",
      "Error: TOO_TO, Message: Did you mean “to see”?, Suggestion: ['to see']\n",
      "Error: EN_A_VS_AN, Message: Use “a” instead of ‘an’ if the following word doesn’t start with a vowel sound, e.g. ‘a sentence’, ‘a university’., Suggestion: ['a']\n",
      "Error: ENGLISH_WORD_REPEAT_RULE, Message: Possible typo: you repeated a word, Suggestion: ['of']\n",
      "Error: MORFOLOGIK_RULE_EN_US, Message: Possible spelling mistake found., Suggestion: ['detect']\n",
      "Error: DO_VBZ, Message: After the auxiliary verb ‘do’, use the base form of the main verb. Did you mean “think”?, Suggestion: ['think']\n",
      "Error: PLEASE_NOT_THAT, Message: Did you mean “note”?, Suggestion: ['note']\n",
      "Error: PM_IN_THE_EVENING, Message: This is redundant. Consider using “P.M.”, Suggestion: ['P.M.']\n",
      "Error: DATE_WEEKDAY, Message: The date 27 June 2017 is not a Thursday, but a Tuesday., Suggestion: []\n"
     ]
    }
   ],
   "source": [
    "from language_tool_python import LanguageTool\n",
    "\n",
    "paragraph = \"LanguageTool offers spell and grammar checking. Just paste your text here and click the ‘Check Text’ button. Click the colored phrases for details on potential errors. or use this text too see an few of of the problems that LanguageTool can detecd. What do you thinks of grammar checkers? Please not that they are not perfect. Style issues get a blue marker: It’s 5 P.M. in the afternoon. The weather was nice on Thursday, 27 June 2017\"\n",
    "\n",
    "def check_grammar(para):\n",
    "    tool = LanguageTool('en-US')\n",
    "    matches = tool.check(para)\n",
    "    return matches\n",
    "\n",
    "grammar_errors = check_grammar(paragraph)\n",
    "print(\"Grammar Errors:\",grammar_errors)\n",
    "\n",
    "for error in grammar_errors:\n",
    "    print(f\"Error: {error.ruleId}, Message: {error.message}, Suggestion: {error.replacements}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Paragraph:\n",
      "He do not likes to go outside. Her writing skill are good, but she could of improved it. The weather is to cold today.\n",
      "\n",
      "Tokenized Sentences:\n",
      "['He do not likes to go outside.', 'Her writing skill are good, but she could of improved it.', 'The weather is to cold today.']\n",
      "\n",
      "Part-of-Speech Tags:\n",
      "[[('He', 'PRP'), ('do', 'VB'), ('not', 'RB'), ('likes', 'VB'), ('to', 'TO'), ('go', 'VB'), ('outside', 'RB'), ('.', '.')], [('Her', 'PRP$'), ('writing', 'NN'), ('skill', 'NN'), ('are', 'VBP'), ('good', 'JJ'), (',', ','), ('but', 'CC'), ('she', 'PRP'), ('could', 'MD'), ('of', 'IN'), ('improved', 'VBN'), ('it', 'PRP'), ('.', '.')], [('The', 'DT'), ('weather', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('cold', 'VB'), ('today', 'NN'), ('.', '.')]]\n",
      "\n",
      "Synonyms and Antonyms:\n",
      "Word: writing, Synonyms: ['writing.n.01', 'writing.n.02', 'writing.n.03', 'writing.n.04', 'writing.n.05', 'write.v.01', 'write.v.02', 'publish.v.03', 'write.v.04', 'write.v.05', 'compose.v.02', 'write.v.07', 'write.v.08', 'spell.v.03', 'write.v.10'], Antonyms: []\n",
      "Word: skill, Synonyms: ['skill.n.01', 'skill.n.02'], Antonyms: []\n",
      "Word: weather, Synonyms: ['weather.n.01', 'weather.v.01', 'weather.v.02', 'weather.v.03', 'weather.v.04', 'upwind.s.01'], Antonyms: []\n",
      "Word: today, Synonyms: ['today.n.01', 'today.n.02', 'nowadays.r.01', 'today.r.02'], Antonyms: []\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def analyze_paragraph(paragraph):\n",
    "    # Tokenize the paragraph into sentences and words\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Part-of-speech tagging\n",
    "    pos_tags = [pos_tag(word) for word in words]\n",
    "\n",
    "    # WordNet analysis (synonyms and antonyms)\n",
    "    synonyms_antonyms = []\n",
    "    for sentence in words:\n",
    "        for word, tag in pos_tag(sentence):\n",
    "            if tag.startswith('N'):\n",
    "                synsets = wordnet.synsets(word)\n",
    "                synonyms = [syn.name() for syn in synsets]\n",
    "                antonyms = [antonym.name() for syn in synsets for antonym in syn.lemmas()[0].antonyms()]\n",
    "                synonyms_antonyms.append((word, synonyms, antonyms))\n",
    "\n",
    "    return sentences, pos_tags, synonyms_antonyms\n",
    "\n",
    "# Example paragraph with intentional grammar mistakes\n",
    "input_paragraph = \"He do not likes to go outside. Her writing skill are good, but she could of improved it. The weather is to cold today.\"\n",
    "\n",
    "sentences, pos_tags, synonyms_antonyms = analyze_paragraph(input_paragraph)\n",
    "\n",
    "print(\"Original Paragraph:\")\n",
    "print(input_paragraph)\n",
    "print(\"\\nTokenized Sentences:\")\n",
    "print(sentences)\n",
    "print(\"\\nPart-of-Speech Tags:\")\n",
    "print(pos_tags)\n",
    "print(\"\\nSynonyms and Antonyms:\")\n",
    "for entry in synonyms_antonyms:\n",
    "    word, synonyms, antonyms = entry\n",
    "    print(f\"Word: {word}, Synonyms: {synonyms}, Antonyms: {antonyms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Paragraph:\n",
      "He do not likes to go outside. Her writing skill are good, but she could of improved it. The weather is to cold today.\n",
      "\n",
      "Corrected Paragraph:\n",
      "He make not wish to travel outside . Her writing skill be good , but she could of better it . The weather be to cold today .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "def correct_paragraph(paragraph):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    corrected_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        pos_tags = pos_tag(words)\n",
    "\n",
    "        # Correct errors in the sentence\n",
    "        corrected_words = []\n",
    "        for word, tag in pos_tags:\n",
    "            # Correct nouns\n",
    "            if tag.startswith('N'):\n",
    "                synsets = wordnet.synsets(word)\n",
    "                if synsets:\n",
    "                    corrected_word = synsets[0].lemmas()[0].name()\n",
    "                else:\n",
    "                    corrected_word = word\n",
    "            # Correct verbs\n",
    "            elif tag.startswith('V'):\n",
    "                synsets = wordnet.synsets(word, pos=wordnet.VERB)\n",
    "                if synsets:\n",
    "                    corrected_word = synsets[0].lemmas()[0].name()\n",
    "                else:\n",
    "                    corrected_word = word\n",
    "            # Default correction\n",
    "            else:\n",
    "                corrected_word = word\n",
    "\n",
    "            corrected_words.append(corrected_word)\n",
    "\n",
    "        corrected_sentence = ' '.join(corrected_words)\n",
    "        corrected_sentences.append(corrected_sentence)\n",
    "\n",
    "    return ' '.join(corrected_sentences)\n",
    "\n",
    "# Example paragraph with intentional grammar mistakes\n",
    "input_paragraph = \"He do not likes to go outside. Her writing skill are good, but she could of improved it. The weather is to cold today.\"\n",
    "\n",
    "corrected_paragraph = correct_paragraph(input_paragraph)\n",
    "\n",
    "print(\"Original Paragraph:\")\n",
    "print(input_paragraph)\n",
    "print(\"\\nCorrected Paragraph:\")\n",
    "print(corrected_paragraph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Her writing skill good'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"her writing skill good\"\n",
    "# is_bad_rule = lambda rule: rule.message == 'Possible spelling mistake found.' and len(rule.replacements) and rule.replacements[0][0].isupper()\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "matches = tool.check(s)\n",
    "# matches = [rule for rule in matches if not is_bad_rule(rule)]\n",
    "language_tool_python.utils.correct(s, matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
      "  Cloning https://github.com/PrithivirajDamodaran/Gramformer.git to c:\\users\\navar\\appdata\\local\\temp\\pip-req-build-wm9k0f9x\n",
      "  Resolved https://github.com/PrithivirajDamodaran/Gramformer.git to commit 23425cd2e98a919384cab6156af8adf1c9d0639a\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Requirement already satisfied: fsspec in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from gramformer==1.0) (2023.12.2)\n",
      "Collecting errant\n",
      "  Downloading errant-3.0.0-py3-none-any.whl (499 kB)\n",
      "Collecting rapidfuzz>=3.4.0\n",
      "  Downloading rapidfuzz-3.6.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "Collecting spacy<4,>=3.2.0\n",
      "  Downloading spacy-3.7.2-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.2-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.10.9)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (20.9)\n",
      "Requirement already satisfied: setuptools in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (69.0.3)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.31.0)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.23.5)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from packaging>=20.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4,>=3.2.0->errant->gramformer==1.0) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (2.0.7)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: colorama in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from jinja2->spacy<4,>=3.2.0->errant->gramformer==1.0) (2.1.3)\n",
      "Collecting Levenshtein==0.23.0\n",
      "  Downloading Levenshtein-0.23.0-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->gramformer==1.0) (6.0.1)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.4.1-cp310-none-win_amd64.whl (277 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\sanu\\eng_rasa_chatbot\\env\\lib\\site-packages (from transformers->gramformer==1.0) (2022.10.31)\n",
      "Building wheels for collected packages: gramformer\n",
      "  Building wheel for gramformer (setup.py): started\n",
      "  Building wheel for gramformer (setup.py): finished with status 'done'\n",
      "  Created wheel for gramformer: filename=gramformer-1.0-py3-none-any.whl size=4504 sha256=386ea7507ec993c83341d0ac1e472553cd7f3c154aab59620137ef319cb44005\n",
      "  Stored in directory: C:\\Users\\navar\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-mgcch0kx\\wheels\\76\\44\\15\\e79b5dc4f5c897b2054e6a8e357f6de157b9554f072a2e56ea\n",
      "Successfully built gramformer\n",
      "Installing collected packages: catalogue, srsly, murmurhash, cymem, wasabi, typer, smart-open, preshed, filelock, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, rapidfuzz, langcodes, jinja2, huggingface-hub, tokenizers, spacy, safetensors, Levenshtein, transformers, sentencepiece, python-Levenshtein, fuzzywuzzy, errant, gramformer\n",
      "Successfully installed Levenshtein-0.23.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 errant-3.0.0 filelock-3.13.1 fuzzywuzzy-0.18.0 gramformer-1.0 huggingface-hub-0.20.2 jinja2-3.1.2 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 python-Levenshtein-0.23.0 rapidfuzz-3.6.1 safetensors-0.4.1 sentencepiece-0.1.99 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.2 tokenizers-0.15.0 transformers-4.36.2 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/PrithivirajDamodaran/Gramformer.git 'C:\\Users\\navar\\AppData\\Local\\Temp\\pip-req-build-wm9k0f9x'\n",
      "WARNING: You are using pip version 21.2.3; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'D:\\Projects\\sanu\\Eng_rasa_chatbot\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/PrithivirajDamodaran/Gramformer.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is not loaded\n"
     ]
    }
   ],
   "source": [
    "from gramformer import Gramformer\n",
    "gf = Gramformer(models=3, use_gpu=True)\n",
    "gf.correct('My camera battery a dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3008, 0.0519, 0.7849],\n",
      "        [0.3906, 0.2869, 0.3256],\n",
      "        [0.2539, 0.1994, 0.8554],\n",
      "        [0.2716, 0.7784, 0.9886],\n",
      "        [0.3175, 0.1615, 0.3149]])\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import TTSettings\n",
    "from happytransformer import HappyTextToText\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSeq2SeqLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSeq2SeqLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m happy_tt \u001b[38;5;241m=\u001b[39m \u001b[43mHappyTextToText\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mT5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprithivida/grammar_error_correcter_v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\sanu\\Eng_rasa_chatbot\\env\\lib\\site-packages\\happytransformer\\happy_text_to_text.py:38\u001b[0m, in \u001b[0;36mHappyTextToText.__init__\u001b[1;34m(self, model_type, model_name, load_path, use_auth_token, trust_remote_code)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madaptor \u001b[38;5;241m=\u001b[39m get_adaptor(model_type)\n\u001b[0;32m     36\u001b[0m model_class \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_class \u001b[38;5;241m=\u001b[39m Text2TextGenerationPipeline\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[1;32md:\\Projects\\sanu\\Eng_rasa_chatbot\\env\\lib\\site-packages\\happytransformer\\happy_transformer.py:34\u001b[0m, in \u001b[0;36mHappyTransformer.__init__\u001b[1;34m(self, model_type, model_name, model_class, load_path, use_auth_token, trust_remote_code)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_path has been deprecated. Provide the load_path to the  model_name parameter instead \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. load_path will be removed in a later version. For now, we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll load the model form the load_path provided.  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m load_path\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_components\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_device()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32md:\\Projects\\sanu\\Eng_rasa_chatbot\\env\\lib\\site-packages\\happytransformer\\happy_transformer.py:67\u001b[0m, in \u001b[0;36mHappyTransformer._get_model_components\u001b[1;34m(self, model_name_path, use_auth_token, trust_remote_code, model_class)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_components\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name_path,  use_auth_token, trust_remote_code, model_class):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# HappyTextClassification is the only class that overwrites\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# this as we need to specify number of labels.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_path, use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code)\n\u001b[1;32m---> 67\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name_path, config\u001b[38;5;241m=\u001b[39mconfig, use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code)\n\u001b[0;32m     68\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_path, use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config, tokenizer, model\n",
      "File \u001b[1;32md:\\Projects\\sanu\\Eng_rasa_chatbot\\env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1288\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1288\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\sanu\\Eng_rasa_chatbot\\env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1267\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForSeq2SeqLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSeq2SeqLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "happy_tt = HappyTextToText(\"T5\",  \"prithivida/grammar_error_correcter_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = TTSettings(do_sample=True, top_k=10, temperature=0.5,  min_length=1, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = happy_tt.generate_text(text, args=settings)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
